{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SMayurgavali/CA-EXPERIMENT/blob/main/Combined_ANN_Models_with_Various_Activation_Functions_%26_Results_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_dataset(file_path: str):\n",
        "    \"\"\"\n",
        "    Loads a dataset from a specified file path.\n",
        "    Supports CSV, Excel (.xls, .xlsx), and JSON formats.\n",
        "    Prints basic information about the loaded dataset.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The full path to the dataset file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded dataset as a pandas DataFrame, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    clean_file_path = file_path.strip()\n",
        "    file_extension = os.path.splitext(clean_file_path)[1].lower().strip()\n",
        "\n",
        "    try:\n",
        "        if file_extension == '.csv':\n",
        "            df = pd.read_csv(clean_file_path)\n",
        "        elif file_extension in ['.xls', '.xlsx']:\n",
        "            df = pd.read_excel(clean_file_path)\n",
        "        elif file_extension == '.json':\n",
        "            df = pd.read_json(clean_file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: '{file_extension}'. Supported types are .csv, .xls, .xlsx, .json.\")\n",
        "            return None\n",
        "        print(\"\\nDataset loaded successfully!\")\n",
        "        print(\"First 5 rows of the dataset:\")\n",
        "        print(df.head())\n",
        "        print(f\"\\nDataset shape: {df.shape}\")\n",
        "        print(\"\\nDataset Info:\")\n",
        "        df.info()\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at '{clean_file_path}'. Please check the path and try again.\")\n",
        "        return None\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Error: The file at '{clean_file_path}' is empty. Please provide a file with data.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while loading the dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    Squashes input values to a range between 0 and 1.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def tanh(z):\n",
        "    \"\"\"\n",
        "    Hyperbolic Tangent (Tanh) activation function.\n",
        "    Squashes input values to a range between -1 and 1.\n",
        "    \"\"\"\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu(z):\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (ReLU) activation function.\n",
        "    Outputs the input directly if positive, otherwise outputs 0.\n",
        "    \"\"\"\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "    \"\"\"\n",
        "    Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n",
        "    Outputs the input directly if positive, otherwise outputs a small multiple (alpha) of the input.\n",
        "    Addresses the 'dying ReLU' problem.\n",
        "    \"\"\"\n",
        "    return np.maximum(alpha * z, z)\n",
        "\n",
        "def softmax_binary_output(z):\n",
        "    \"\"\"\n",
        "    Softmax activation function adapted for binary classification.\n",
        "    Equivalent to sigmoid for a two-class problem.\n",
        "    \"\"\"\n",
        "    return np.exp(z) / (np.exp(z) + 1)\n",
        "\n",
        "def get_activation_function(name: str):\n",
        "    \"\"\"\n",
        "    Retrieves an activation function by its name.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the activation function (e.g., 'sigmoid', 'relu').\n",
        "\n",
        "    Returns:\n",
        "        function: The corresponding activation function.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported activation function name is provided.\n",
        "    \"\"\"\n",
        "    activation_functions = {\n",
        "        'sigmoid': sigmoid,\n",
        "        'tanh': tanh,\n",
        "        'relu': relu,\n",
        "        'leaky_relu': leaky_relu,\n",
        "        'softmax': softmax_binary_output\n",
        "    }\n",
        "    func = activation_functions.get(name.lower())\n",
        "    if func is None:\n",
        "        raise ValueError(f\"Unsupported activation function '{name}'. \"\n",
        "                         f\"Available options: {', '.join(activation_functions.keys())}.\")\n",
        "    return func\n",
        "\n",
        "def train_model(X_train, y_train, activation_func, learning_rate=0.005, num_iterations=10000):\n",
        "    \"\"\"\n",
        "    Trains a single-layer perceptron model using gradient descent.\n",
        "\n",
        "    Args:\n",
        "        X_train (np.ndarray): Training features.\n",
        "        y_train (np.ndarray): Training target labels (0 or 1).\n",
        "        activation_func (function): The activation function to use.\n",
        "        learning_rate (float): The learning rate for gradient descent.\n",
        "        num_iterations (int): The number of training iterations.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the learned weights (np.ndarray), bias (float), and loss history.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = X_train.shape\n",
        "    weights = np.zeros(n_features)\n",
        "    bias = 0\n",
        "    loss_history = []\n",
        "\n",
        "    print(f\"\\nStarting training with {activation_func.__name__.replace('_binary_output', '')} activation...\")\n",
        "    print(f\"Learning Rate: {learning_rate}, Iterations: {num_iterations}\")\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        linear_model = np.dot(X_train, weights) + bias\n",
        "        y_predicted_raw = activation_func(linear_model)\n",
        "\n",
        "        dw = (1 / n_samples) * np.dot(X_train.T, (y_predicted_raw - y_train))\n",
        "        db = (1 / n_samples) * np.sum(y_predicted_raw - y_train)\n",
        "\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "        if activation_func in [sigmoid, softmax_binary_output]:\n",
        "            y_predicted_clipped = np.clip(y_predicted_raw, 1e-10, 1 - 1e-10)\n",
        "            current_loss = -np.mean(y_train * np.log(y_predicted_clipped) + (1 - y_train) * np.log(1 - y_predicted_clipped))\n",
        "        else:\n",
        "            current_loss = np.mean((y_predicted_raw - y_train)**2)\n",
        "\n",
        "        loss_history.append(current_loss)\n",
        "\n",
        "        if (i + 1) % (num_iterations // 10) == 0:\n",
        "            print(f\"Iteration {i+1}/{num_iterations}, Loss: {current_loss:.6f}\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return weights, bias, loss_history\n",
        "\n",
        "def predict_classes(X, weights, bias, activation_func, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Makes binary class predictions using the trained model.\n",
        "    \"\"\"\n",
        "    linear_model = np.dot(X, weights) + bias\n",
        "    y_predicted_raw = activation_func(linear_model)\n",
        "    y_predicted_classes = (y_predicted_raw >= threshold).astype(int)\n",
        "    return y_predicted_classes\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance using various classification metrics.\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n--- Evaluation Metrics for {model_name} ---\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(f\"  True Negative (TN): {cm[0,0]}\")\n",
        "    print(f\"  False Positive (FP): {cm[0,1]}\")\n",
        "    print(f\"  False Negative (FN): {cm[1,0]}\")\n",
        "    print(f\"  True Positive (TP): {cm[1,1]}\")\n",
        "    return accuracy, precision, recall, f1, cm\n",
        "\n",
        "def save_model(weights, bias, activation_name, scaler, file_path=\"model_params.pkl\"):\n",
        "    \"\"\"Saves the trained model parameters and scaler to a file.\"\"\"\n",
        "    model_data = {\n",
        "        'weights': weights,\n",
        "        'bias': bias,\n",
        "        'activation_name': activation_name,\n",
        "        'scaler': scaler\n",
        "    }\n",
        "    with open(file_path, 'wb') as f:\n",
        "        pickle.dump(model_data, f)\n",
        "    print(f\"\\nModel parameters saved successfully to '{file_path}'\")\n",
        "\n",
        "def load_model(file_path=\"model_params.pkl\"):\n",
        "    \"\"\"Loads a trained model from a file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            model_data = pickle.load(f)\n",
        "        print(f\"\\nModel parameters loaded successfully from '{file_path}'\")\n",
        "        return model_data['weights'], model_data['bias'], model_data['activation_name'], model_data['scaler']\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Model file not found at '{file_path}'.\")\n",
        "        return None, None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the model: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸš€ Advanced Perceptron Model for Binary Classification\")\n",
        "    print(\"This script trains and evaluates a single-layer perceptron with multiple activation functions.\")\n",
        "    print(\"It includes data preprocessing, evaluation metrics, and model saving/loading.\")\n",
        "\n",
        "    dataset_path = input(\"Enter the full path to your dataset file: \").strip()\n",
        "\n",
        "    if not dataset_path:\n",
        "        print(\"No path entered. Exiting.\")\n",
        "    else:\n",
        "        my_data = load_dataset(dataset_path)\n",
        "\n",
        "        if my_data is not None:\n",
        "            try:\n",
        "                # Basic data cleaning: drop rows with any missing values\n",
        "                my_data = my_data.dropna()\n",
        "\n",
        "                # Separate features (X) and target (y)\n",
        "                X = my_data.iloc[:, :-1].select_dtypes(include=np.number).values\n",
        "                y = my_data.iloc[:, -1].astype(int).values\n",
        "\n",
        "                if not np.all(np.isin(y, [0, 1])):\n",
        "                    print(\"Error: The target variable (last column) is not binary (0 or 1). Exiting.\")\n",
        "                    exit()\n",
        "\n",
        "                # Feature Scaling\n",
        "                scaler = StandardScaler()\n",
        "                X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "                # Split data into training and testing sets\n",
        "                X_train, X_test, y_train, y_test = train_test_split(\n",
        "                    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        "                )\n",
        "                print(f\"\\nData split into training ({len(X_train)} samples) and testing ({len(X_test)} samples).\")\n",
        "\n",
        "                print(\"\\nAvailable Activations: Sigmoid, Tanh, ReLU, Leaky_ReLU, Softmax\")\n",
        "                activation_choices_str = input(\"Enter desired activation functions, separated by commas: \").strip()\n",
        "                selected_activations = [choice.strip() for choice in activation_choices_str.split(',') if choice.strip()]\n",
        "\n",
        "                if not selected_activations:\n",
        "                    print(\"No activation functions selected. Exiting.\")\n",
        "                    exit()\n",
        "\n",
        "                try:\n",
        "                    user_learning_rate = float(input(\"Enter learning rate (e.g., 0.005): \") or \"0.005\")\n",
        "                    user_num_iterations = int(input(\"Enter number of iterations (e.g., 10000): \") or \"10000\")\n",
        "                except ValueError:\n",
        "                    print(\"Invalid input. Using defaults (0.005, 10000).\")\n",
        "                    user_learning_rate = 0.005\n",
        "                    user_num_iterations = 10000\n",
        "\n",
        "                results = {}\n",
        "                best_activation = None\n",
        "                highest_accuracy = -1.0\n",
        "\n",
        "                for act_name in selected_activations:\n",
        "                    try:\n",
        "                        chosen_activation_func = get_activation_function(act_name)\n",
        "                        print(f\"\\n--- Running model with {act_name} activation ---\")\n",
        "\n",
        "                        weights, bias, loss_history = train_model(\n",
        "                            X_train, y_train, chosen_activation_func,\n",
        "                            learning_rate=user_learning_rate,\n",
        "                            num_iterations=user_num_iterations\n",
        "                        )\n",
        "\n",
        "                        # Plotting the loss history\n",
        "                        plt.figure()\n",
        "                        plt.plot(loss_history)\n",
        "                        plt.title(f'Loss History for {act_name} Activation')\n",
        "                        plt.xlabel('Iterations')\n",
        "                        plt.ylabel('Loss')\n",
        "                        plt.grid(True)\n",
        "                        plt.show()\n",
        "\n",
        "                        y_pred_test = predict_classes(X_test, weights, bias, chosen_activation_func)\n",
        "                        accuracy, precision, recall, f1, cm = evaluate_model(y_test, y_pred_test, model_name=f\"{act_name} (Test Set)\")\n",
        "\n",
        "                        results[act_name] = {\n",
        "                            \"weights\": weights, \"bias\": bias, \"test_accuracy\": accuracy,\n",
        "                            \"test_precision\": precision, \"test_recall\": recall, \"test_f1\": f1,\n",
        "                            \"test_confusion_matrix\": cm\n",
        "                        }\n",
        "\n",
        "                        if accuracy > highest_accuracy:\n",
        "                            highest_accuracy = accuracy\n",
        "                            best_activation = act_name\n",
        "                            best_weights = weights\n",
        "                            best_bias = bias\n",
        "\n",
        "                    except ValueError as ve:\n",
        "                        print(f\"Error during training with {act_name}: {ve}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"An unexpected error occurred during training with {act_name}: {e}\")\n",
        "\n",
        "\n",
        "                print(\"\\n--- Model Training Summary ---\")\n",
        "                for act_name, metrics in results.items():\n",
        "                    print(f\"\\n{act_name} Activation:\")\n",
        "                    print(f\"  Test Accuracy: {metrics['test_accuracy']:.2f}%\")\n",
        "                    print(f\"  Test Precision: {metrics['test_precision']:.4f}\")\n",
        "                    print(f\"  Test Recall: {metrics['test_recall']:.4f}\")\n",
        "                    print(f\"  Test F1-Score: {metrics['test_f1']:.4f}\")\n",
        "\n",
        "                if best_activation:\n",
        "                    print(f\"\\nBest performing activation on test set: {best_activation} (Accuracy: {highest_accuracy:.2f}%)\")\n",
        "\n",
        "                    save_model(best_weights, best_bias, best_activation, scaler)\n",
        "\n",
        "                    # Example of loading and using the saved model\n",
        "                    loaded_weights, loaded_bias, loaded_activation_name, loaded_scaler = load_model()\n",
        "\n",
        "                    if loaded_weights is not None:\n",
        "                        loaded_activation_func = get_activation_function(loaded_activation_name)\n",
        "                        # Assuming you want to predict on the test set again with the loaded model\n",
        "                        X_test_scaled = loaded_scaler.transform(my_data.iloc[:, :-1].select_dtypes(include=np.number).values) # Use original data for scaling\n",
        "                        y_pred_loaded = predict_classes(X_test_scaled, loaded_weights, loaded_bias, loaded_activation_func)\n",
        "                        print(\"\\nEvaluation of the Loaded Model:\")\n",
        "                        evaluate_model(my_data.iloc[:, -1].astype(int).values, y_pred_loaded, model_name=\"Loaded Model (Full Data)\") # Evaluate on full data to match scaling\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during data processing or model execution: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Advanced Perceptron Model for Binary Classification\n",
            "This script trains and evaluates a single-layer perceptron with multiple activation functions.\n",
            "It includes data preprocessing, evaluation metrics, and model saving/loading.\n",
            "Enter the full path to your dataset file: /content/data.csv\n",
            "\n",
            "Dataset loaded successfully!\n",
            "First 5 rows of the dataset:\n",
            "   feature_A,feature_B,target_label  Unnamed: 1  Unnamed: 2\n",
            "0                                 6          20           1\n",
            "1                                 4          23           0\n",
            "2                                 5          21           1\n",
            "3                                 2          26           1\n",
            "4                                 3          28           0\n",
            "\n",
            "Dataset shape: (6, 3)\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6 entries, 0 to 5\n",
            "Data columns (total 3 columns):\n",
            " #   Column                            Non-Null Count  Dtype\n",
            "---  ------                            --------------  -----\n",
            " 0   feature_A,feature_B,target_label  6 non-null      int64\n",
            " 1   Unnamed: 1                        6 non-null      int64\n",
            " 2   Unnamed: 2                        6 non-null      int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 276.0 bytes\n",
            "\n",
            "Data split into training (4 samples) and testing (2 samples).\n",
            "\n",
            "Available Activations: Sigmoid, Tanh, ReLU, Leaky_ReLU, Softmax\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXWSnjdK0CT5",
        "outputId": "90fab92c-a4a5-435c-8719-a58be6bc4afe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}